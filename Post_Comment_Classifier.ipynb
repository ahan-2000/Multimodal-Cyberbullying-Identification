{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "id": "8W1B7KudO2lz",
    "outputId": "130837ac-caa6-4173-a0ba-04644c4a5f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-06 09:30:58--  https://www.dropbox.com/s/uiurqzh02bo3dru/vine_labeled_cyberbullying_data.csv\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.1, 2620:100:6032:1::a27d:5201\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/uiurqzh02bo3dru/vine_labeled_cyberbullying_data.csv [following]\n",
      "--2019-08-06 09:30:59--  https://www.dropbox.com/s/raw/uiurqzh02bo3dru/vine_labeled_cyberbullying_data.csv\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc4bca977c6e6b3286b0f85f68c3.dl.dropboxusercontent.com/cd/0/inline/AmEQpBHh4B_T5qCzM0UwmaVHoa2oHJ_iB5CqwX2XR7G1nt8QJBuMSgMi-Q9PZ7QluQRiU9Jwg1uB3j4XBKhTgGiPQz6Gw92gq3pM0jo2GUvo7mrIcTg_uiwplD-aVR21EFE/file# [following]\n",
      "--2019-08-06 09:30:59--  https://uc4bca977c6e6b3286b0f85f68c3.dl.dropboxusercontent.com/cd/0/inline/AmEQpBHh4B_T5qCzM0UwmaVHoa2oHJ_iB5CqwX2XR7G1nt8QJBuMSgMi-Q9PZ7QluQRiU9Jwg1uB3j4XBKhTgGiPQz6Gw92gq3pM0jo2GUvo7mrIcTg_uiwplD-aVR21EFE/file\n",
      "Resolving uc4bca977c6e6b3286b0f85f68c3.dl.dropboxusercontent.com (uc4bca977c6e6b3286b0f85f68c3.dl.dropboxusercontent.com)... 162.125.82.6, 2620:100:6032:6::a27d:5206\n",
      "Connecting to uc4bca977c6e6b3286b0f85f68c3.dl.dropboxusercontent.com (uc4bca977c6e6b3286b0f85f68c3.dl.dropboxusercontent.com)|162.125.82.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13249071 (13M) [text/plain]\n",
      "Saving to: ‘vine_labeled_cyberbullying_data.csv’\n",
      "\n",
      "vine_labeled_cyberb 100%[===================>]  12.63M  6.86MB/s    in 1.8s    \n",
      "\n",
      "2019-08-06 09:31:01 (6.86 MB/s) - ‘vine_labeled_cyberbullying_data.csv’ saved [13249071/13249071]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/uiurqzh02bo3dru/vine_labeled_cyberbullying_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "id": "DNHPBC1QO4oz",
    "outputId": "17133ad6-abe9-493d-a04e-fa8940ab379b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-06 09:31:04--  https://www.dropbox.com/s/dbl1hdln6e4d5hv/comment_embedding.csv\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.1, 2620:100:6032:1::a27d:5201\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/dbl1hdln6e4d5hv/comment_embedding.csv [following]\n",
      "--2019-08-06 09:31:04--  https://www.dropbox.com/s/raw/dbl1hdln6e4d5hv/comment_embedding.csv\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uccd90e17a45e34448e7dc23e4ab.dl.dropboxusercontent.com/cd/0/inline/AmF9fb4R-Mx12HyPUKdfgQwY9mQMJHuZxwiQ5BGiOUO2T_olXbf-AbkA04LXdHnCo2_DCRe3jlm0Xv66vA1ISi6X-TBuzqRlyby2jK2LuXJEwWBpYfEhAhSfbvUjjGDhiF0/file# [following]\n",
      "--2019-08-06 09:31:05--  https://uccd90e17a45e34448e7dc23e4ab.dl.dropboxusercontent.com/cd/0/inline/AmF9fb4R-Mx12HyPUKdfgQwY9mQMJHuZxwiQ5BGiOUO2T_olXbf-AbkA04LXdHnCo2_DCRe3jlm0Xv66vA1ISi6X-TBuzqRlyby2jK2LuXJEwWBpYfEhAhSfbvUjjGDhiF0/file\n",
      "Resolving uccd90e17a45e34448e7dc23e4ab.dl.dropboxusercontent.com (uccd90e17a45e34448e7dc23e4ab.dl.dropboxusercontent.com)... 162.125.82.6, 2620:100:6032:6::a27d:5206\n",
      "Connecting to uccd90e17a45e34448e7dc23e4ab.dl.dropboxusercontent.com (uccd90e17a45e34448e7dc23e4ab.dl.dropboxusercontent.com)|162.125.82.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 490817455 (468M) [text/plain]\n",
      "Saving to: ‘comment_embedding.csv’\n",
      "\n",
      "comment_embedding.c 100%[===================>] 468.08M  50.6MB/s    in 9.3s    \n",
      "\n",
      "2019-08-06 09:31:14 (50.5 MB/s) - ‘comment_embedding.csv’ saved [490817455/490817455]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/dbl1hdln6e4d5hv/comment_embedding.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cvr-Viiq1xp6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Input, concatenate, Dropout\n",
    "from keras.layers import BatchNormalization, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jsog1ldj2cdv"
   },
   "outputs": [],
   "source": [
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_G5tDkeFOmPR"
   },
   "outputs": [],
   "source": [
    "data_shape = (10, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjOKa3a71xqH"
   },
   "outputs": [],
   "source": [
    "class ModelDef:\n",
    "    def __init__(self, input_shape=(99, 512), hidden_states=256):\n",
    "        self.input_shape = input_shape\n",
    "        self.hidden_states = hidden_states\n",
    "    \n",
    "    def build_resnet(self):\n",
    "        inp = Input(shape=self.input_shape)\n",
    "        \n",
    "        x = Bidirectional(LSTM(self.hidden_states, return_sequences=True, kernel_initializer='glorot_uniform'))(inp)\n",
    "        x1 = LeakyReLU(0.2)(x)\n",
    "        \n",
    "        x = Bidirectional(LSTM(self.hidden_states, return_sequences=True, kernel_initializer='glorot_uniform'))(x1)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        \n",
    "        x = concatenate([x1, x])\n",
    "        \n",
    "        x = Bidirectional(LSTM(self.hidden_states, return_sequences=True, kernel_initializer='glorot_uniform'))(x)\n",
    "        x1 = LeakyReLU(0.2)(x)\n",
    "        \n",
    "        x = Bidirectional(LSTM(self.hidden_states, return_sequences=True, kernel_initializer='glorot_uniform'))(x1)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        \n",
    "        x = concatenate([x1, x])\n",
    "        \n",
    "        \n",
    "        x = Bidirectional(LSTM(self.hidden_states, return_sequences=False, kernel_initializer='glorot_uniform'))(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        \n",
    "        x = Dense(512, kernel_initializer='glorot_uniform')(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid')(x)\n",
    "        \n",
    "        model = Model(inputs=inp, outputs=x)\n",
    "        return model\n",
    "    \n",
    "    def build_bilstm(self):\n",
    "        inp = Input(shape=self.input_shape)\n",
    "        \n",
    "        x = Bidirectional(LSTM(self.hidden_states, return_sequences=True, kernel_initializer='glorot_uniform'))(inp)\n",
    "        x = Bidirectional(LSTM(self.hidden_states, return_sequences=True, kernel_initializer='glorot_uniform'))(x)\n",
    "        \n",
    "        x1 = GlobalMaxPooling1D(x)\n",
    "        x2 = GlobalAveragePooling1D(x)\n",
    "        \n",
    "        x = concatenate([x1, x2])\n",
    "        \n",
    "        x = Dense(512, kernel_initializer='glorot_uniform')(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid')(x)\n",
    "        \n",
    "        model = Model(inputs=inp, outputs=x)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wiHX5NrW1xqP",
    "outputId": "4a9cae38-9e00-483b-c3fd-0c59d6310059"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0806 09:32:20.187740 140368490915712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0806 09:32:20.227793 140368490915712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0806 09:32:20.236059 140368490915712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0806 09:32:24.000184 140368490915712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0806 09:32:24.026079 140368490915712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0806 09:32:24.034392 140368490915712 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 10, 512)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 10, 512)      1574912     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 10, 512)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 10, 512)      1574912     leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 10, 512)      0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10, 1024)     0           leaky_re_lu_1[0][0]              \n",
      "                                                                 leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 10, 512)      2623488     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 10, 512)      0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 10, 512)      1574912     leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 10, 512)      0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 10, 1024)     0           leaky_re_lu_3[0][0]              \n",
      "                                                                 leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 512)          2623488     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 512)          0           bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            513         leaky_re_lu_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 10,234,881\n",
      "Trainable params: 10,234,881\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ModelDef(data_shape)\n",
    "model = model.build_resnet()\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YwN1aKke1xqb"
   },
   "outputs": [],
   "source": [
    "def convert_str_to_array(array_str):\n",
    "    array_str = array_str.replace('[', '')\n",
    "    array_str = array_str.replace(']', '')\n",
    "    array_str = array_str.replace(' ', '')\n",
    "    return np.fromstring(array_str, sep=', ')\n",
    "\n",
    "class ReadData:\n",
    "    def __init__(self, dataset, embedding_path, data_shape=(99, 512), train_val_split=0.1):\n",
    "        print('Reading Dataset ..')\n",
    "        self.dataset = pd.read_csv(dataset)\n",
    "        self.dataset = self.dataset.sample(frac=1.0).reset_index(drop=True)\n",
    "        print('Done.')\n",
    "        print('Reading Embeddings ..')\n",
    "        self.embeddings = pd.read_csv(embedding_path)\n",
    "        self.embeddings.embedding = self.embeddings.embedding.apply(convert_str_to_array)\n",
    "        print('Done.')\n",
    "        \n",
    "        self.training = self.dataset.head(int(len(self.dataset)*(1-train_val_split))).reset_index(drop=True)\n",
    "        self.validation = self.dataset.tail(int(len(self.dataset)*(train_val_split))).reset_index(drop=True)\n",
    "        \n",
    "        self.train_size = len(self.training)\n",
    "        self.val_size = len(self.validation)\n",
    "        self.data_shape = data_shape\n",
    "        \n",
    "    def sent2vec(self, sent, embed_size=512):\n",
    "        embed = self.embeddings[self.embeddings.comment == sent]\n",
    "        if embed.empty:\n",
    "            return np.zeros(embed_size)\n",
    "        \n",
    "        return list(embed.embedding)[0]\n",
    "        \n",
    "    def post_embedding(self, comments, max_len=99, dim=512):\n",
    "        vector = []\n",
    "        for i, comment in enumerate(comments):\n",
    "            if i < max_len:\n",
    "                try:\n",
    "                    vec = self.sent2vec(comment, dim)\n",
    "                    vector.append(vec)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "            \n",
    "\n",
    "        vectors = []\n",
    "        vectors += list(vector)\n",
    "        padding_len = max_len - len(vectors)\n",
    "        for _ in range(padding_len):\n",
    "            vectors.append(np.zeros(dim))\n",
    "\n",
    "        return np.array(vectors)\n",
    "    \n",
    "    def get_comments(self, row, i):\n",
    "        comments = []\n",
    "        #print(row)\n",
    "        for j in range(1, 100):\n",
    "            comm = row['column{}'.format(j)][i]\n",
    "            try:\n",
    "                comm = [x.strip() for x in re.findall('\\:\\:.*?(.*)\\(created', comm, re.MULTILINE | re.DOTALL)]\n",
    "                #break\n",
    "                if comm == []:\n",
    "                    break\n",
    "                else:\n",
    "                    comments += comm\n",
    "            except TypeError:\n",
    "                pass\n",
    "            \n",
    "        return comments\n",
    "    \n",
    "    def generator_val(self, batch_size=64):\n",
    "        while True:\n",
    "            no_batches = int(self.val_size/batch_size)\n",
    "            for i in range(no_batches):\n",
    "                start_index = i*batch_size\n",
    "                batch_x, batch_y = [], []\n",
    "                for j in range(i, i+batch_size):\n",
    "                    #print(j)\n",
    "                    #print(self.training.head())\n",
    "                    row = self.validation.iloc[[j]]\n",
    "                    comments = self.get_comments(row, j)\n",
    "                    x = self.post_embedding(comments, self.data_shape[0], self.data_shape[1])\n",
    "                    y = []\n",
    "                    if row['question2'][j] == 'noneBll':\n",
    "                        y.append(0)\n",
    "                    else:\n",
    "                        y.append(1)\n",
    "\n",
    "                    batch_x.append(x)\n",
    "                    batch_y.append(y)\n",
    "\n",
    "                x, y = np.array(batch_x), np.array(batch_y)\n",
    "                #y = np.reshape(y, (y.shape[0], y.shape[2]))\n",
    "                yield x, y\n",
    "\n",
    "    def generator(self, batch_size=64):\n",
    "        while True:\n",
    "            no_batches = int(self.train_size/batch_size)\n",
    "            for i in range(no_batches):\n",
    "                start_index = i*batch_size\n",
    "                batch_x, batch_y = [], []\n",
    "                for j in range(i, i+batch_size):\n",
    "                    #print(j)\n",
    "                    #print(self.training.head())\n",
    "                    row = self.training.iloc[[j]]\n",
    "                    comments = self.get_comments(row, j)\n",
    "                    x = self.post_embedding(comments, self.data_shape[0], self.data_shape[1])\n",
    "                    y = []\n",
    "                    if row['question2'][j] == 'noneBll':\n",
    "                        y.append(0)\n",
    "                    else:\n",
    "                        y.append(1)\n",
    "\n",
    "                    batch_x.append(x)\n",
    "                    batch_y.append(y)\n",
    "\n",
    "                x, y = np.array(batch_x), np.array(batch_y)\n",
    "                #y = np.reshape(y, (y.shape[0], y.shape[2]))\n",
    "                yield x, y\n",
    "    \n",
    "    def next_batch(self, i, batch_size=64):\n",
    "        batch_x, batch_y = [], []\n",
    "        for j in range(i, i+batch_size):\n",
    "            #print(j)\n",
    "            #print(self.training.head())\n",
    "            row = self.training.iloc[[j]]\n",
    "            comments = self.get_comments(row, j)\n",
    "            x = self.post_embedding(comments, self.data_shape[0], self.data_shape[1])\n",
    "            y = []\n",
    "            if row['question2'][j] == 'noneBll':\n",
    "                y.append(0)\n",
    "            else:\n",
    "                y.append(1)\n",
    "                \n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y)\n",
    "\n",
    "        x, y = np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GOUbqXvz3Sha"
   },
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "log_dir = 'post_classifier_{}-comments'.format(data_shape[0])\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "AI9KmbyI1xql",
    "outputId": "2dbc3e42-861d-492f-da94-28717528dc95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Dataset ..\n",
      "Done.\n",
      "Reading Embeddings ..\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'vine_labeled_cyberbullying_data.csv'\n",
    "reader = ReadData(dataset_path, 'comment_embedding.csv', data_shape=data_shape, train_val_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zV8Xiknb3Igm"
   },
   "outputs": [],
   "source": [
    "train_generator = reader.generator(batch_size)\n",
    "val_generator = reader.generator_val(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E8UaWHBtjETT"
   },
   "outputs": [],
   "source": [
    "# for x,y in train_generator:\n",
    "#     print(x.shape, y.shape, x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "YG7Gb5zD1xq4",
    "outputId": "1ddfbb92-5b85-4fa1-f110-3af2046fdd99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 776 samples and Validating on 194 samples.\n"
     ]
    }
   ],
   "source": [
    "logging = TrainValTensorBoard(log_dir=log_dir)\n",
    "checkpoint = ModelCheckpoint(os.path.join(log_dir, 'ep{epoch:03d}-val_loss{val_loss:.3f}-val_acc{val_acc:.3f}.h5'),\n",
    "        monitor='val_acc', save_weights_only=True, save_best_only=True, period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=3, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=1)\n",
    "\n",
    "print('Training on {} samples and Validating on {} samples.'.format(reader.train_size, reader.val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "49S13Y341xrB",
    "outputId": "2d280320-636c-4cc6-f674-23bdf604e358"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0806 09:32:39.646702 140368490915712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0806 09:32:43.655260 140368490915712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0806 09:32:43.656829 140368490915712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "48/48 [==============================] - 49s 1s/step - loss: 0.3383 - acc: 0.8880 - val_loss: 0.3310 - val_acc: 0.8698\n",
      "Epoch 2/20\n",
      "48/48 [==============================] - 42s 870ms/step - loss: 0.0668 - acc: 0.9844 - val_loss: 0.5229 - val_acc: 0.8490\n",
      "Epoch 3/20\n",
      "48/48 [==============================] - 43s 903ms/step - loss: 0.0228 - acc: 0.9948 - val_loss: 1.9721 - val_acc: 0.6406\n",
      "Epoch 4/20\n",
      " 6/48 [==>...........................] - ETA: 46s - loss: 0.2487 - acc: 0.9375"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator=train_generator, steps_per_epoch=int(reader.train_size/batch_size),\n",
    "                    validation_data=val_generator, epochs=epochs, validation_steps=int(reader.val_size/batch_size),\n",
    "                    callbacks=[logging, checkpoint, reduce_lr, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2AIt2KZ34C6"
   },
   "outputs": [],
   "source": [
    "#!zip -r post_classifier.zip post_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9upoKCSvIbgb"
   },
   "outputs": [],
   "source": [
    "#!du -hs post_classifier/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qsNBGRETIs0I"
   },
   "outputs": [],
   "source": [
    "#!cp post_classifier/ep* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VtvMTDDP6cQ3"
   },
   "outputs": [],
   "source": [
    "'''# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once per notebook.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# Download a file based on its file ID.\n",
    "#\n",
    "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
    "file_id = '1MNmLad9X0OQDa3H1nzjRKc5ckzxmEt_S'\n",
    "downloaded = drive.CreateFile({'id': file_id})\n",
    "print('Downloaded content \"{}\"'.format(downloaded.GetContentFile('comment_embedding.csv')))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QXjZFAoO9_vk"
   },
   "outputs": [],
   "source": [
    "#!wget https://www.dropbox.com/s/lxy5x0h7m7xcasn/vine_labeled_cyberbullying_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Z-4rmqs-JpO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Post-Comment-Classifier.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
